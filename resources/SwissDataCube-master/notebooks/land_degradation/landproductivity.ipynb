{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends.Earth productivity Notebook v2 <a name=\"top\"></a>\n",
    "\n",
    "Conversion of [Trends.Earth productivity](http://trends.earth/docs/en/background/understanding_indicators.html#productivity) [script](https://github.com/ConservationInternational/landdegradation/blob/master/landdegradation/productivity.py) into a Jupyter Notebook.\n",
    "\n",
    "<img src=\"http://trends.earth/docs/en/_images/indicator_15_3_1_prod_subindicators.png\" alt=\"\" width=\"600\"/>   \n",
    "   \n",
    "    \n",
    "- [Load mean annual NDVI](#load_data) (and quality dataset) from saved netcdf file - section\n",
    "\n",
    "  generated by [BC_NDVI_annual_mean.ipynb](BC_NDVI_annual_mean.ipynb) and stored in work_space.\n",
    "    \n",
    "\n",
    "- [Visualize mean annual NDVI through time (OPTIONAL)](#visualize_data) section (and quality dataset) - section\n",
    "\n",
    "\n",
    "- [Initiate output dataset](#init_ds) section\n",
    "\n",
    "\n",
    "- [Compute trajectory](#trajectory) - section\n",
    "    \n",
    "<a href=\"http://trends.earth/docs/en/background/understanding_indicators.html#productivity-trajectory\"><img src=\"http://trends.earth/docs/en/_images/lp_traj_flow.PNG\" alt=\"\" width=\"800\"/></a>\n",
    "\n",
    "- [Compute performance](#performance) - section\n",
    "\n",
    "<a href=\"http://trends.earth/docs/en/background/understanding_indicators.html#productivity-performance\"><img src=\"http://trends.earth/docs/en/_images/lp_perf_flow.PNG\" alt=\"\" width=\"800\"/></a>\n",
    "\n",
    "- [Compute state](#state) - section\n",
    "\n",
    "<a href=\"http://trends.earth/docs/en/background/understanding_indicators.html#productivity-state\"><img src=\"http://trends.earth/docs/en/_images/lp_state_flow.PNG\" alt=\"\" width=\"800\"/></a>\n",
    "\n",
    "- [Combinine productivity indicators](#combine) - section\n",
    "\n",
    "<a href=\"http://trends.earth/docs/en/background/understanding_indicators.html#combining-productivity-indicators\"><img src=\"http://trends.earth/docs/en/_images/lp_aggregation.PNG\" alt=\"\" width=\"800\"/></a>\n",
    "\n",
    "- [Visualize productivity (OPTIONAL)](#visualize) - section\n",
    "\n",
    "**Requirements:**\n",
    "- ./swiss_utils/data_cube_utilities/sdc_utilities.py\n",
    "- Land Cover: ./auxiliary_data/CH_lc_TE.tif (SWISS only)\n",
    "- Soil Taxonomy: ./auxiliary_data/CH_soil_tax_TE.tif (Swiss only)\n",
    "- NDVI annual mean netcdf (stored in work_path) from ITI_NDVI_annual_mean_v2.ipynb\n",
    "\n",
    "**Changelog:**\n",
    "- better management of nodata\n",
    "- TAXNWRB_250m_ll.tif and ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992_2015-v2.0.7.tif respectively replaced by CH_soil_tax_TE.tif and CH_lc_TE.tif (back-processed from Trends.Earth plugin output for all Switzerland) as it appears the plugin is not using files mentionned in documentation.\n",
    "- minor changes\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import calendar\n",
    "import uuid\n",
    "import rasterio\n",
    "import gdal\n",
    "import osr\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib import colors\n",
    "from pandas import DataFrame\n",
    "\n",
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    basemaps,\n",
    "    basemap_to_tiles,\n",
    "    ImageOverlay,\n",
    "    DrawControl,\n",
    "    LayersControl\n",
    ")\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "from utils.data_cube_utilities.dc_display_map import display_map, _degree_to_zoom_level\n",
    "from utils.data_cube_utilities.dc_utilities import write_single_band_png_from_xr\n",
    "\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import printandlog, load_multi_clean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration section\n",
    "\n",
    "work_path = '/datacube/ui_results/notebook/BC_switzerland_0112_LS58'\n",
    "\n",
    "start_year = 2001\n",
    "bl_end_year = 2012 # Baseline end\n",
    "tg_start_year = 2013 # Target start\n",
    "end_year = 2015\n",
    "\n",
    "# Performance calculation data\n",
    "lc_path = './auxiliary_data/CH_lc_TE.tif'\n",
    "soil_path = './auxiliary_data/CH_soil_tax_TE.tif'\n",
    "\n",
    "log_name = 'BC_TendsEarth_productivity_v2.log'\n",
    "user_mail = 'bruno.chatenoux@unepgrid.ch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export to geotiff\n",
    "\n",
    "EXT2DRIV_CONVERSION = {\n",
    "    \"tif\": \"GTiff\",\n",
    "    \"nc\": \"netCDF\",\n",
    "    \"img\": \"HFA\"\n",
    "}\n",
    "\n",
    "NP2GDAL_CONVERSION = {\n",
    "      \"uint8\": 1,\n",
    "      \"int8\": 1,\n",
    "      \"uint16\": 2,\n",
    "      \"int16\": 3,\n",
    "      \"uint32\": 4,\n",
    "      \"int32\": 5,\n",
    "      \"float32\": 6,\n",
    "      \"uint64\": 7,\n",
    "      \"int64\": 7,\n",
    "      \"float64\": 7,\n",
    "      \"complex64\": 10,\n",
    "      \"complex128\": 11,\n",
    "    }\n",
    "\n",
    "def da2raster(da, rast_name, nodata_val = None, md_rast = None, md_band = None, opt = []):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Convert an xarray.DataArray into a proper Geotiff (no shift, CRS (epsg:4326), metadata and nodata value)\n",
    "    -----\n",
    "    Input:\n",
    "      da: xarray.DataArray\n",
    "      rast_name: string\n",
    "          Output file name (could be .tif. img or .nc, but the last one will not manage the metadata)\n",
    "      nodata: number (optional)\n",
    "      md_rast: dictionnary (optional)\n",
    "          raster metadata e.g. {'long_name':'productivity indicators', 'configuartion':'Landsat 5,7 and 8 2001-2015'}\n",
    "      md_band: dictionnary (optional)\n",
    "          band metadata e.g. {'long_name':'linear trend', 'units':'NDVI/year'}\n",
    "      opt: list\n",
    "          gdal option (depends on used raster format) e.g. ['COMPRESS=DEFLATE']\n",
    "          see https://www.gdal.org/formats_list.html for options per format\n",
    "    Output:\n",
    "      raster file\n",
    "    \"\"\"\n",
    "    \n",
    "    # identify GDAL Driver using rast_name extent\n",
    "    driv = EXT2DRIV_CONVERSION[rast_name.split('.')[-1]]\n",
    "    \n",
    "    # identify da GDAL data type\n",
    "    # source: https://borealperspectives.wordpress.com/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/\n",
    "    gdaltype = NP2GDAL_CONVERSION[da.dtype.name]\n",
    "    \n",
    "    # set band name\n",
    "    band_name = da.name if da.name is not None else 'noname'\n",
    "    \n",
    "    # compute georeferencing parameters\n",
    "    cols = da.shape[1]\n",
    "    rows = da.shape[0]\n",
    "    rasterOrigin = (da.longitude.values.min(), da.latitude.values.max()) # Top-Left\n",
    "    pixelWidth = (da.longitude.values.max() - da.longitude.values.min()) / (cols - 1)\n",
    "    pixelHeight = (da.latitude.values.max() - da.latitude.values.min()) / (rows - 1)\n",
    "    origin_left = rasterOrigin[0] - pixelWidth / 2\n",
    "    origin_top = rasterOrigin[1] + pixelHeight / 2\n",
    "    \n",
    "    # Create the geotiff\n",
    "    driver = gdal.GetDriverByName(driv)\n",
    "    outRaster = driver.Create(rast_name, cols, rows, 1, gdaltype, options=opt)\n",
    "    if md_rast:\n",
    "        outRaster.SetMetadata(md_rast)\n",
    "    outRaster.SetGeoTransform((origin_left, pixelWidth, 0, origin_top, 0, -pixelHeight))\n",
    "    outband = outRaster.GetRasterBand(1)\n",
    "    outband.WriteArray(da.values)\n",
    "    outband.SetDescription(band_name)\n",
    "    if md_band:\n",
    "        outband.SetMetadata(md_band)\n",
    "    if nodata_val:\n",
    "        outband.SetNoDataValue(nodata_val)\n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromEPSG(4326)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mean annual NDVI and quality dataset <a name=\"load_data\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list files in work_path (will not work with subfolders)\n",
    "for dirpath, dirnames, filenames in os.walk(work_path):\n",
    "    break\n",
    "\n",
    "# Compare requested and available years\n",
    "years = []\n",
    "noyears = []\n",
    "for year in range(start_year, end_year + 1):\n",
    "    if 'NDVI_%s.nc' % (year) in filenames:\n",
    "        years.append(year)\n",
    "    else:\n",
    "        print('Year %s is not available' % (year))\n",
    "        noyears.append(year)\n",
    "len_years = len(years)\n",
    "        \n",
    "# Load and merge .nc files\n",
    "NDVI = [None]*len_years\n",
    "for year_ind, year in enumerate(years):\n",
    "    print(year)\n",
    "    da = xr.open_dataset('%s/NDVI_%s.nc' % (work_path, year))\n",
    "    NDVI[year_ind] = da\n",
    "    \n",
    "NDVI = xr.concat(NDVI, dim='time')\n",
    "NDVI.coords['time'] = years\n",
    "\n",
    "# Add missing years with nodata\n",
    "for year in noyears:\n",
    "    nan_NDVI = NDVI.isel(time=0).where(not NDVI.isel(time=0), 0)\n",
    "    nan_NDVI['mean'] = nan_NDVI['mean'].where(not nan_NDVI)\n",
    "    nan_NDVI.coords['time'] = year\n",
    "    nan_NDVI = nan_NDVI.expand_dims('time')\n",
    "    NDVI = xr.merge([NDVI, nan_NDVI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize mean annual NDVI and quality dataset through time (OPTIONAL) <a name=\"visualize_data\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)\n",
    "\n",
    "**_!!! Full section to be commented in case of large dataset !!!_**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = NDVI['mean'].to_dataframe(name = 'ndvi_mean')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "cax = df.boxplot(column='ndvi_mean', by='time', figsize = (12, 8))\n",
    "for label in cax.xaxis.get_ticklabels()[::2]:\n",
    "    label.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = NDVI['count'].to_dataframe(name = 'ndvi_count')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "cax = df.boxplot(column='ndvi_count', by='time', figsize = (12, 8))\n",
    "for label in cax.xaxis.get_ticklabels()[::2]:\n",
    "    label.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = NDVI['qual'].to_dataframe(name = 'ndvi_qual')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "cax = df.boxplot(column='ndvi_qual', by='time', figsize = (12, 8))\n",
    "for label in cax.xaxis.get_ticklabels()[::2]:\n",
    "    label.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"NDVI mean\")\n",
    "NDVI['mean'].plot(x='longitude', y='latitude', col='time', col_wrap=5,\n",
    "                  cmap = colors.LinearSegmentedColormap.from_list('ndvi', ['darkblue','blue','lightblue','lightgreen','darkgreen'], N=256),\n",
    "                  vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"NDVI mean - Count (number of pixels with data)\")\n",
    "NDVI['count'].plot(x='longitude', y='latitude', col='time', col_wrap=5,\n",
    "                   cmap = colors.LinearSegmentedColormap.from_list('pc', ['lightcoral','lightgreen','green','darkgreen'], N=256),\n",
    "                   vmin = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"NDVI mean - Quality (percentage of pixels with data)\")\n",
    "NDVI['qual'].plot(x='longitude', y='latitude', col='time', col_wrap=5,\n",
    "               cmap = colors.LinearSegmentedColormap.from_list('pc', ['lightcoral','lightgreen','green','darkgreen'], N=256),\n",
    "               vmin = 0, vmax = 100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "groups = NDVI['mean'].groupby('time')\n",
    "df = DataFrame()\n",
    "for name, group in groups:\n",
    "    df[name] = np.histogram(group.values[0], bins=np.arange(0, 1.1, 0.05))[0]\n",
    "\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(df, interpolation=None, aspect='auto',\n",
    "                 cmap = colors.LinearSegmentedColormap.from_list('', ['lightgrey','green'], N=256))\n",
    "ax.invert_yaxis()\n",
    "plt.xticks([0, end_year - start_year], [start_year, end_year])\n",
    "plt.yticks([0, len(df) - 1], [0, 1])\n",
    "# fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate output dataset <a name=\"init_ds\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# quality metadata for geotiff export\n",
    "md_traj = {'description':'productivity-quality indicators using Landsat 5,7 and 8 over the period 2001-2015'}\n",
    "\n",
    "# Compute statistic over le full period\n",
    "ndvi_mean = NDVI['mean']\n",
    "ndvi_avg = ndvi_mean.mean(dim=['time'])\n",
    "ndvi_avg.name = 'ndvi_avg'\n",
    "productivity = ndvi_avg.to_dataset(name = 'ndvi_avg').astype(np.float32)\n",
    "da2raster(ndvi_avg, '%s/qual_ndvi_avg.tif' % (work_path),\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'NDVI mean', 'units':'NDVI'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "ndvi_count = NDVI['count'].sum(dim=['time'])\n",
    "ndvi_count.name = 'ndvi_count'\n",
    "productivity = productivity.merge(ndvi_count.to_dataset(name = 'ndvi_count').astype(np.uint16))\n",
    "da2raster(ndvi_count, '%s/qual_ndvi_count.tif' % (work_path),\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'data count', 'units':'none'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "ndvi_pc = np.rint(ndvi_count / np.rint(NDVI['count'] / (NDVI['qual'] / 100)).sum(dim=['time']) * 100).astype(np.uint8)\n",
    "ndvi_pc.name = 'ndvi_pc'\n",
    "productivity = productivity.merge(ndvi_pc.to_dataset(name = 'ndvi_pc'))\n",
    "da2raster(ndvi_pc, '%s/qual_ndvi_pc.tif' % (work_path),\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'data percentage', 'units':'percentage'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del ndvi_count\n",
    "del ndvi_pc\n",
    "\n",
    "print(productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute trajectory <a name=\"trajectory\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# trajectory metadata for geotiff export\n",
    "md_traj = {'description':'productivity-trajectory indicators using Landsat 5,7 and 8 over the period 2001-2015'}  \n",
    "\n",
    "# Calculate mk_trend\n",
    "def mann_kendall(x, alpha = 0.05):\n",
    "    n = len(x)\n",
    "\n",
    "    # calculate S \n",
    "    s = 0\n",
    "    for k in range(n-1):\n",
    "        for j in range(k+1,n):    \n",
    "            v = np.sign(x[j] - x[k])\n",
    "            v = v.fillna(0) # replace nan with 0\n",
    "            s += v\n",
    "    return s.astype(np.int16)\n",
    "\n",
    "mk_trend = mann_kendall(NDVI['mean']).compute()\n",
    "mk_trend.name = 'mk_trend'\n",
    "productivity = productivity.merge(mk_trend.to_dataset(name = 'mk_trend'))\n",
    "da2raster(mk_trend, '%s/traj_mk_trend.tif' % (work_path),\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'Mann-Kendal', 'units':'none'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "def da_linreg_params(y, dim = 'time'):\n",
    "    x = y.where(np.isnan(y), y[dim]) # attribute time to pixel with values\n",
    "\n",
    "    mean_x = x.mean(dim=dim)\n",
    "    mean_y = y.mean(dim=dim)\n",
    "    mean_xx = (x * x).mean(dim=dim)\n",
    "    mean_xy = (x * y).mean(dim=dim)\n",
    "\n",
    "    s = ((mean_x * mean_y) - mean_xy) / ((mean_x * mean_x) - mean_xx)\n",
    "    \n",
    "    i = mean_y - mean_x * s\n",
    "    \n",
    "    return s, i\n",
    "\n",
    "lin_trend, intercept = da_linreg_params(NDVI['mean'])\n",
    "\n",
    "lin_trend.name = 'lin_trend'\n",
    "productivity = productivity.merge(lin_trend.to_dataset(name = 'lin_trend'))\n",
    "da2raster(lin_trend, '%s/traj_lin_trend.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'linear trend', 'units':'NDVI/year'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "# Calulate Mann-Kendall significance\n",
    "def get_kendall_coef(n, level=95):\n",
    "    # The minus 4 is because the indexing below for a sample size of 4\n",
    "    assert(n >= 4)\n",
    "    n = n - 4\n",
    "    coefs = {90: [4, 6, 7, 9, 10, 12, 15, 17, 18, 22, 23, 27, 28, 32, 35, 37, 40, 42,\n",
    "                  45, 49, 52, 56, 59, 61, 66, 68, 73, 75, 80, 84, 87, 91, 94, 98, 103,\n",
    "                  107, 110, 114, 119, 123, 128, 132, 135, 141, 144, 150, 153, 159,\n",
    "                  162, 168, 173, 177, 182, 186, 191, 197, 202],\n",
    "             95: [4, 6, 9, 11, 14, 16, 19, 21, 24, 26, 31, 33, 36, 40, 43, 47, 50, 54,\n",
    "                  59, 63, 66, 70, 75, 79, 84, 88, 93, 97, 102, 106, 111, 115, 120,\n",
    "                  126, 131, 137, 142, 146, 151, 157, 162, 168, 173, 179, 186, 190,\n",
    "                  197, 203, 208, 214, 221, 227, 232, 240, 245, 251, 258],\n",
    "             99: [6, 8, 11, 18, 22, 25, 29, 34, 38, 41, 47, 50, 56, 61, 65, 70, 76, 81,\n",
    "                  87, 92, 98, 105, 111, 116, 124, 129, 135, 142, 150, 155, 163, 170,\n",
    "                  176, 183, 191, 198, 206, 213, 221, 228, 236, 245, 253, 260, 268,\n",
    "                  277, 285, 294, 302, 311, 319, 328, 336, 345, 355, 364]}\n",
    "    return coefs[level][n]\n",
    "\n",
    "kendall90 = get_kendall_coef(len_years, 90)\n",
    "kendall95 = get_kendall_coef(len_years, 95)\n",
    "kendall99 = get_kendall_coef(len_years, 99)\n",
    "\n",
    "# np.logical_and used as instead of np.all as is 30 to 40% faster\n",
    "signif = np.full((mk_trend.shape[0], mk_trend.shape[1]), 9999)\n",
    "signif = np.where(np.logical_and(lin_trend.values < 0, abs(mk_trend.values) >= kendall90), -1, signif)\n",
    "signif = np.where(np.logical_and(lin_trend.values < 0, abs(mk_trend.values) >= kendall95), -2, signif)\n",
    "signif = np.where(np.logical_and(lin_trend.values < 0, abs(mk_trend.values) >= kendall99), -3, signif)\n",
    "signif = np.where(np.logical_and(lin_trend.values > 0, abs(mk_trend.values) >= kendall90), 1, signif)\n",
    "signif = np.where(np.logical_and(lin_trend.values > 0, abs(mk_trend.values) >= kendall95), 2, signif)\n",
    "signif = np.where(np.logical_and(lin_trend.values > 0, abs(mk_trend.values) >= kendall99), 3, signif)\n",
    "signif = np.where(abs(mk_trend.values) <= kendall90, 0, signif)\n",
    "signif = np.where(abs(lin_trend.values) <= 0.001, 0, signif)\n",
    "\n",
    "# Convert signif to a propper DataArray\n",
    "signif = xr.DataArray(signif, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "signif = signif.assign_coords(latitude=mk_trend.latitude, longitude=mk_trend.longitude)\n",
    "signif.name = 'signif'\n",
    "productivity = productivity.merge(signif.to_dataset(name = 'signif'))\n",
    "da2raster(signif, '%s/traj_signif.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'significance', 'units':'category'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del mk_trend\n",
    "del lin_trend\n",
    "del signif\n",
    "\n",
    "print(productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute performance <a name=\"performance\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# performance metadata for geotiff export\n",
    "md_traj = {'description':'productivity-performance indicators using Landsat 5,7 and 8 over the period 2001-2015'}\n",
    "\n",
    "# Resample and load Land Cover and Soil taxonomy\n",
    "def clip_resample_load(xd, tif_path):\n",
    "    \"\"\"\n",
    "    clip, resample and load a geotiff (tif_path) to overlay a given xarray.Dataset or DataArray (xd)\n",
    "    in EPSG 4326 CRS (reproject if needed).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xd: xarray.Dataset or DataArray\n",
    "    tif_path: str\n",
    "        The string filepath to a GeoTIFF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate temporary file name\n",
    "    tmp_name = str(uuid.uuid4()) + '.tif'\n",
    "    \n",
    "    # clip and resample with gdal\n",
    "    cmd = 'gdalwarp -te {:.20f} {:.20f} {:.20f} {:.20f} -ts {} {} -t_srs EPSG:4326 -co COMPRESS=LZW -wm 4096 -multi {} {}' \\\n",
    "      .format(xd.longitude.values.min(), xd.latitude.values.min(), xd.longitude.values.max(), xd.latitude.values.max(),\n",
    "              len(xd.longitude), len(xd.latitude),\n",
    "              tif_path, tmp_name)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # load\n",
    "    with rasterio.open(tmp_name, driver='GTiff') as dst:\n",
    "        data_np_arr = dst.read()\n",
    "        dst.close()\n",
    "    os.remove(tmp_name)\n",
    "        \n",
    "    da = xr.DataArray(data_np_arr, dims=['time', 'latitude', 'longitude'])\n",
    "    da = da.assign_coords(time = range(data_np_arr.shape[0]),\n",
    "                                      latitude=xd.latitude,\n",
    "                                      longitude=xd.longitude)\n",
    "    \n",
    "    return da\n",
    "\n",
    "# LOAD LAND COVER\n",
    "# # V1 using ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992_2015-v2.0.7.tif\n",
    "# lc = clip_resample_load(NDVI['mean'], lc_path)\n",
    "\n",
    "# # geotiff is composed of one band per year for the period 1992-2015\n",
    "# # Assign years to time (index so far) and keep only the appropriate period\n",
    "# lc.coords['time'] = range(1992, 2015 + 1)\n",
    "# lc = lc[lc.coords['time'].isin(range(start_year, end_year + 1))]\n",
    "\n",
    "# # reclassify lc (first year) to ipcc class\n",
    "# # some idx categories are missning (0 values and 200 cat added and reclassified as 0)\n",
    "# idx = np.asarray([10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 160, 170, 110, 130, 180, 190, 120, 121, 122, 140, 150, 151, 152, 153, 200, 201, 202, 210, 220])\n",
    "# val = np.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0])\n",
    "# lookup_array = np.zeros(idx.max() + 1)\n",
    "# lookup_array[idx] = val\n",
    "# lookup_array = lookup_array[lc.isel(time=0).values.astype(np.uint8)].astype(np.uint8)\n",
    "# lc_t0 = xr.DataArray(lookup_array, dims=['latitude', 'longitude'])\n",
    "# lc_t0 = lc_t0.assign_coords(latitude=ndvi_mean.latitude,\n",
    "#                             longitude=ndvi_mean.longitude)\n",
    "# v2 using CH_lc_TE.tif\n",
    "lc_t0 = clip_resample_load(NDVI['mean'], lc_path)\n",
    "lc_t0 = lc_t0.where(lc_t0 != -32768, 0).isel(time=0)\n",
    "\n",
    "# LOAD SOIL TAXONOMY\n",
    "# # V1 using TAXNWRB_250m_ll.tif\n",
    "# soil_tax = clip_resample_load(NDVI['mean'], soil_path)\n",
    "# soil_tax = soil_tax.where(soil_tax.values != 255)\n",
    "\n",
    "# v2 using CH_soil_tax_TE.tif\n",
    "soil_tax = clip_resample_load(NDVI['mean'], soil_path)\n",
    "soil_tax = soil_tax.where(soil_tax != -32768, 0).isel(time=0)\n",
    "\n",
    "\n",
    "# define unit of analysis as the intersect of soil_tax_usda and land cover\n",
    "# # V1\n",
    "# units = soil_tax.isel(time=0).astype(np.uint64).values * 100 + lc_t0\n",
    "# V2\n",
    "units = (soil_tax.astype(np.uint64).values * 100 + lc_t0).astype(np.uint16)\n",
    "units.name = 'units'\n",
    "# V1\n",
    "productivity = productivity.merge(units.to_dataset(name = 'units')) # V1\n",
    "# V2\n",
    "# da2raster(units, '%s/perf_units.tif' % (work_path),\n",
    "da2raster(units.where(units != 0), '%s/perf_units.tif' % (work_path),\n",
    "          nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'Soil/LC units', 'units':'id'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "# compute 90th statistique per unit\n",
    "ndvi_id = ndvi_avg.to_dataset(name = 'ndvi')\n",
    "ndvi_id = ndvi_id.merge(units.to_dataset(name = 'units'))\n",
    "perc90 = ndvi_id.groupby('units').reduce(np.quantile, q=0.9)\n",
    "\n",
    "# remap perc90 on units\n",
    "idx = perc90.units.values\n",
    "val = (perc90.ndvi.values * 100000).astype(np.uint64)\n",
    "lookup_array = np.zeros(int(idx.max() + 1))\n",
    "lookup_array[idx] = val\n",
    "# # V1\n",
    "# lookup_array = lookup_array[units]\n",
    "# V2\n",
    "lookup_array = lookup_array[units.values]\n",
    "raster_perc = xr.DataArray(lookup_array / 100000, dims=['latitude', 'longitude'])\n",
    "raster_perc = raster_perc.assign_coords(latitude=ndvi_mean.latitude,\n",
    "                                        longitude=ndvi_mean.longitude)\n",
    "ndvi_id = ndvi_id.merge(raster_perc.to_dataset(name = 'perc90'))\n",
    "\n",
    "# compute degradation\n",
    "ndvi_id['obs_ratio'] = ndvi_id.ndvi / ndvi_id.perc90\n",
    "productivity = productivity.merge(ndvi_id['obs_ratio'].to_dataset(name = 'obs_ratio'))\n",
    "# # V1\n",
    "# da2raster(ndvi_id['obs_ratio'], '%s/perf_ratio.tif' % (work_path),\n",
    "# V2\n",
    "da2raster(ndvi_id['obs_ratio'].where(units != 0), '%s/perf_ratio.tif' % (work_path),\n",
    "          nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'NDVI average / perc 90 ratio', 'units':'percentage'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "# compute degradation using obs_ratio = 0.5 as a threshold\n",
    "deg = np.zeros((ndvi_mean.shape[1], ndvi_mean.shape[2]))\n",
    "deg = np.where(ndvi_id.obs_ratio <= 0.5, -1, 0)\n",
    "deg = xr.DataArray(deg, dims=['latitude', 'longitude']).astype(np.int16) # -1 becomes nodata when using np.int8\n",
    "deg = deg.assign_coords(latitude=ndvi_id.latitude, longitude=ndvi_id.longitude)\n",
    "deg.name = 'degradation'\n",
    "productivity = productivity.merge(deg.to_dataset(name = 'deg'))\n",
    "# # V1\n",
    "# da2raster(deg, '%s/perf_deg.tif' % (work_path),\n",
    "# V2\n",
    "da2raster(deg.where(units != 0), '%s/perf_deg.tif' % (work_path),\n",
    "          nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'degradation', 'units':'category'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del units\n",
    "del ndvi_id\n",
    "del deg\n",
    "\n",
    "print(productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute state <a name=\"state\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# performance metadata for geotiff export\n",
    "md_traj = {'description':'productivity-state indicators using Landsat 5,7 and 8 using the period 2001-2012 as baseline and the period 2013-2015 as a target'}\n",
    "\n",
    "# Define periods and create subsets of mean annual NDVI values for each period\n",
    "bl_ndvi_range = ndvi_mean.sel(time=slice(start_year,bl_end_year)) # bl_ (Baseline period)\n",
    "tg_ndvi_range = ndvi_mean.sel(time=slice(tg_start_year,end_year)) # tg_ (Target period)\n",
    "\n",
    "# Calculate min and max for baseline period:\n",
    "bl_ndvi_min = bl_ndvi_range.min(dim=['time'])\n",
    "bl_ndvi_max = bl_ndvi_range.max(dim=['time'])\n",
    "\n",
    "# Extend the min and max by 5 % for the baseline period\n",
    "bl_extended_perc_5 = bl_ndvi_min - ((bl_ndvi_max - bl_ndvi_min) * 0.05)\n",
    "bl_extended_perc_105 = bl_ndvi_max + ((bl_ndvi_max - bl_ndvi_min) * 0.05)\n",
    "\n",
    "# Generate extreme values (+/- 5% extended values) with the baseline mean NDVI subset in new\n",
    "# datAarray with new \"ind\" coordinate\n",
    "bl_extended = [None]*(len(bl_ndvi_range['time']) + 2)\n",
    "years = range(start_year, bl_end_year + 1)\n",
    "for year_ind, year in enumerate(years):\n",
    "    bl_extended[year_ind] = bl_ndvi_range.sel(time=years[year_ind])\n",
    "    bl_extended[year_ind] = bl_extended[year_ind].assign_coords(ind=year_ind).drop('time')\n",
    "p5 = year_ind+1\n",
    "p105 = year_ind+2\n",
    "bl_extended[p5] = bl_extended_perc_5.assign_coords(ind=p5)\n",
    "bl_extended[p105] = bl_extended_perc_105.assign_coords(ind=p105)\n",
    "bl_extended = xr.concat(bl_extended, dim ='ind')\n",
    "\n",
    "# Define the percentiles of the extended range of annual mean ndvi values in the baseline period\n",
    "# using np.quantile on \"cleaned\" data (dropna) instead of xr.quantile or np.naquantile (both ~300x faster)\n",
    "# bl_quant =  bl_extended.quantile(quantiles, dim='ind')\n",
    "# bl_quant = np.nanquantile(bl_extended, quantiles, axis = 0)\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "bl_quant = np.quantile(bl_extended.dropna('ind', how='all'), quantiles, axis = 0)\n",
    "bl_quant = xr.DataArray(bl_quant, dims=['quantile', 'latitude', 'longitude'])\n",
    "bl_quant = bl_quant.assign_coords(quantile = quantiles,\n",
    "                                  latitude=bl_extended.latitude,\n",
    "                                  longitude=bl_extended.longitude)\n",
    "\n",
    "# Calculate mean NDVI for baseline and target periods\n",
    "bl_ndvi_mean = bl_ndvi_range.mean(dim='time').rename('NDVI_mean')\n",
    "tg_ndvi_mean = tg_ndvi_range.mean(dim='time').rename('NDVI_mean')\n",
    "\n",
    "# Reclassify the baseline mean ndvis based on the quantiles\n",
    "bl_classes = np.full((bl_ndvi_mean.shape[0], bl_ndvi_mean.shape[1]), 9999)\n",
    "bl_classes=np.where(bl_ndvi_mean <= bl_quant.sel(quantile=0.1), 1, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.1), 2, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.2), 3, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.3), 4, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.4), 5, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.5), 6, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.6), 7, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.7), 8, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.8), 9, bl_classes)\n",
    "bl_classes=np.where(bl_ndvi_mean > bl_quant.sel(quantile=0.9), 10, bl_classes)\n",
    "\n",
    "# Reclassify the target mean ndvis based on the quantiles\n",
    "tg_classes = np.full((tg_ndvi_mean.shape[0], tg_ndvi_mean.shape[1]), 9999)\n",
    "tg_classes=np.where(tg_ndvi_mean <= bl_quant.sel(quantile=0.1), 1, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.1), 2, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.2), 3, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.3), 4, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.4), 5, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.5), 6, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.6), 7, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.7), 8, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.8), 9, tg_classes)\n",
    "tg_classes=np.where(tg_ndvi_mean > bl_quant.sel(quantile=0.9), 10, tg_classes)\n",
    "\n",
    "# Convert the new arrays to proper DataArrays (cause they're missing coordinates after the calculation)\n",
    "bl_classes = xr.DataArray(bl_classes, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "bl_classes = bl_classes.assign_coords(latitude=bl_ndvi_mean.latitude, longitude=bl_ndvi_mean.longitude)\n",
    "bl_classes.name = 'bl_classes'\n",
    "productivity = productivity.merge(bl_classes.to_dataset(name = 'bl_classes'))\n",
    "da2raster(bl_classes, '%s/stat_bl_classes.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'baseline NDVI percentile', 'units':'class'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "tg_classes = xr.DataArray(tg_classes, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "tg_classes = tg_classes.assign_coords(latitude=tg_ndvi_mean.latitude, longitude=tg_ndvi_mean.longitude)\n",
    "tg_classes.name = 'tg_classes'\n",
    "productivity = productivity.merge(tg_classes.to_dataset(name = 'tg_classes'))\n",
    "da2raster(tg_classes, '%s/stat_tg_classes.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'target NDVI percentile', 'units':'class'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "# Compare the difference between the baseline and the target mean NDVI values based on the quantile classes\n",
    "# with the exception of subtle changes of mean NDVI value (<0.01) which gets a \"0\"\n",
    "classes_chg = tg_classes - bl_classes\n",
    "classes_chg = np.where(abs(bl_ndvi_mean - tg_ndvi_mean) <= 0.01, 0, classes_chg)\n",
    "\n",
    "# Convert the new arrays to proper DataArrays (cause they're missing coordinates after the calculation)\n",
    "classes_chg = xr.DataArray(classes_chg, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "classes_chg = classes_chg.assign_coords(latitude=bl_classes.latitude, longitude=bl_classes.longitude)\n",
    "classes_chg.name = 'classes_chg'\n",
    "productivity = productivity.merge(classes_chg.to_dataset(name = 'classes_chg'))\n",
    "da2raster(classes_chg, '%s/stat_classes_chg.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'NDVI percentile difference', 'units':'class'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "\n",
    "# Normalize the classes_chg dataarray in order to consider that <=-2 is degradation\n",
    "classes_chg_norm = np.full((classes_chg.shape[0], classes_chg.shape[1]), 9999)\n",
    "classes_chg_norm = np.where(classes_chg < 2, 0, classes_chg_norm)\n",
    "classes_chg_norm = np.where(classes_chg <= -2, -1, classes_chg_norm)\n",
    "classes_chg_norm = np.where(classes_chg >=2, 1, classes_chg_norm)\n",
    "\n",
    "# Convert the new arrays to proper DataArrays (cause they're missing coordinates after the calculation)\n",
    "classes_chg_norm = xr.DataArray(classes_chg_norm, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "classes_chg_norm = classes_chg_norm.assign_coords(latitude=classes_chg.latitude, longitude=classes_chg.longitude)\n",
    "classes_chg_norm.name = 'classes_chg_norm'\n",
    "productivity = productivity.merge(classes_chg_norm.to_dataset(name = 'classes_chg_norm'))\n",
    "da2raster(classes_chg_norm, '%s/stat_classes_chg_norm.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'State', 'units':'category'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del bl_classes\n",
    "del tg_classes\n",
    "del classes_chg\n",
    "del classes_chg_norm\n",
    "\n",
    "print(productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine productivity indicators <a name=\"combine\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Final calculation of productivity\n",
    "# 3 categories(improving = 1, degrading = -3, stable = 0)\n",
    "# plus 2 optional categories (stable but stressed = -1, early signs of decline = -2)\n",
    "\n",
    "combination = np.full((NDVI.dims['latitude'], NDVI.dims['longitude']), 9999)\n",
    "\n",
    "# Addressing....\n",
    "# lines 1 to 5\n",
    "combination = np.where(np.logical_and(productivity.signif.values > 1,\n",
    "                                      np.logical_not(productivity.signif.values == 9999)), 1, combination)\n",
    "# lines 13 to 18\n",
    "combination = np.where((productivity.signif.values < -1), -3, combination)\n",
    "# lines 7 to 9\n",
    "combination = np.where(np.logical_and(productivity.signif.values < 2,\n",
    "                                      productivity.signif.values > -2), 0, combination)\n",
    "# lines 6, 12 (and 18 !)\n",
    "combination = np.where(np.logical_and(productivity.deg.values < 0,\n",
    "                                      productivity.classes_chg.values < -1), -3, combination)\n",
    "\n",
    "# Separation in 2 versions of productivity (3 classes or 5 classes)\n",
    "#Prod_3\n",
    "# lines 10 and 11\n",
    "combination_3 = np.where(np.logical_and(np.logical_and(combination == 0,\n",
    "                                                       np.logical_or(productivity.deg.values < 0,\n",
    "                                                                     productivity.classes_chg.values < -1)),\n",
    "                                        np.logical_not(productivity.classes_chg.values > 1)), -3, combination)\n",
    "\n",
    "#Prod_5\n",
    "# line 6 (different than prod_3)\n",
    "combination_5 = np.where(np.logical_and(np.logical_and(productivity.signif.values > 1,\n",
    "                                                       np.logical_not(productivity.signif.values == 9999)),\n",
    "                                        np.logical_and(productivity.classes_chg.values < -1,\n",
    "                                                       productivity.deg.values < 0)), 0, combination)\n",
    "# line 10 (needed to add a fake clause for the second logical_and)\n",
    "combination_5 = np.where(np.logical_and(np.logical_and(combination == 0,\n",
    "                                                       productivity.deg.values == -1),\n",
    "                                        np.logical_and(productivity.classes_chg.values < 2,\n",
    "                                                       productivity.classes_chg.values > -2)), -1, combination_5)\n",
    "# line 11 \n",
    "combination_5 = np.where(np.logical_and(combination == 0,\n",
    "                                        np.logical_and(productivity.classes_chg.values < -1,\n",
    "                                                       productivity.deg.values == 0)), -2, combination_5)\n",
    "\n",
    "# Convert propper DataArray\n",
    "md_traj = {'description':'3 classes productivity using Landsat 5,7 and 8 using the period 2001-2012 as baseline and the period 2013-2015 as a target'}\n",
    "\n",
    "combination_3 = xr.DataArray(combination_3, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "combination_3 = combination_3.assign_coords(latitude=NDVI.latitude, longitude=NDVI.longitude)\n",
    "combination_3.name = 'combination_3'\n",
    "productivity = productivity.merge(combination_3.to_dataset(name = 'combination_3'))\n",
    "da2raster(combination_3, '%s/prod_combination_3.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'combination_3', 'units':'categories'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del combination_3\n",
    "combination_5 = xr.DataArray(combination_5, dims=['latitude', 'longitude']).astype(np.int16)\n",
    "combination_5 = combination_5.assign_coords(latitude=NDVI.latitude, longitude=NDVI.longitude)\n",
    "combination_5.name = 'combination_5'\n",
    "productivity = productivity.merge(combination_5.to_dataset(name = 'combination_5'))\n",
    "da2raster(combination_5, '%s/prod_combination_5.tif' % (work_path), nodata_val = 9999,\n",
    "          md_rast = md_traj,\n",
    "          md_band = {'long_name':'combination_5', 'units':'categories'},\n",
    "          opt = ['COMPRESS=DEFLATE'])\n",
    "del combination_5\n",
    "\n",
    "print(productivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printandlog('ALL DONE\\n', log_name)\n",
    "\n",
    "if user_mail != '':\n",
    "    os.system(\"mail -s 'Script BC_TrendsEarth_productivity_v2 completed' %s\" % (user_mail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize productivity (OPTIONAL)<a name=\"visualize\"></a>[<div style=\"text-align: right; font-size: 24px\"> &#x1F51D; </div>](#top)\n",
    "\n",
    "**_!!! Full section to be commented in case of large dataset !!!_**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Visualize variables DEV\n",
    "\n",
    "import math\n",
    "\n",
    "vars = productivity.var()\n",
    "ncol = 3\n",
    "nrow = math.ceil(len(vars) / ncol)\n",
    "i = 1\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplots_adjust(hspace=1, wspace=1)\n",
    "for v in vars:\n",
    "    plt.subplot(nrow, ncol, i)\n",
    "    productivity[v].where(productivity[v] != 9999).plot()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def display_array_and_sample_point(da, var, cmap):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Display a colored single variable (integer format) of an xarray.Dataset on a map\n",
    "      and allow the user to select a point\n",
    "    -----\n",
    "    Input:\n",
    "      da: xarray.Dataset\n",
    "      var: variable to display (must be integer)\n",
    "      cmap: colormap to apply (GRASS r.colors ot ESRI HDR color table files (.clr) format)\n",
    "    Output:\n",
    "      m: map to interact with\n",
    "      dc: draw control\n",
    "    Usage:\n",
    "      View, interact and point a location to be used later on\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check inputs\n",
    "    assert 'Dataset' in str(type(da)), \"da must be an xarray.Dataset\"\n",
    "    assert var in da, \"%s variable is not available in the xarray.Dataset (da)\" % var\n",
    "    for item in cmap:\n",
    "        if len(item) != 2: sys.exit('cmap must be a list of list [<value>, <color>] and we have %s' % item)\n",
    "    \n",
    "    # Initiate a fresh temporary working directory\n",
    "    os.system(\"rm -rf ./tmp_display\")\n",
    "    os.mkdir(\"tmp_display\")\n",
    "    \n",
    "    \n",
    "    # Convert dataArray into positive integer (needed by png format) and adapt cmap \n",
    "    da = da[var]\n",
    "    min_val = da.values.min()\n",
    "    if min_val < 0:\n",
    "        da = (da - min_val).astype(np.uint8)\n",
    "        for item in cmap:\n",
    "            item[0] = item[0] - min_val\n",
    "            \n",
    "    with open('./tmp_display/cmap.clr', 'w') as f:\n",
    "        for item in cmap:\n",
    "            f.write(\"%s\\n\" % '\\t'.join(str(e) for e in item))\n",
    "    \n",
    "    # Convert dataArray to temporary png\n",
    "    # png becomes faded dur to gdaldem use (only gdal function to deal with colormap)\n",
    "    # and nor -exact_color_entry or -nearest_color_entry improves it !\n",
    "    png_name = './tmp_display/' + str(uuid.uuid4()) + '.png'\n",
    "    write_single_band_png_from_xr(png_name, da.to_dataset(name = var), var,\n",
    "                                  no_data = 0, color_scale = './tmp_display/cmap.clr')\n",
    "    \n",
    "    # Display\n",
    "    latitude = (da.latitude.values.min(), da.latitude.values.max())\n",
    "    longitude = (da.longitude.values.min(), da.longitude.values.max())\n",
    "    \n",
    "    margin = -0.5\n",
    "    zoom_bias = 0\n",
    "    lat_zoom_level = _degree_to_zoom_level(margin = margin, *latitude ) + zoom_bias\n",
    "    lon_zoom_level = _degree_to_zoom_level(margin = margin, *longitude) + zoom_bias\n",
    "    zoom = min(lat_zoom_level, lon_zoom_level) \n",
    "    center = [np.mean(latitude), np.mean(longitude)]\n",
    "    m = Map(center=center, zoom=zoom)\n",
    "    \n",
    "    # http://leaflet-extras.github.io/leaflet-providers/preview/\n",
    "    esri = basemap_to_tiles(basemaps.Esri.WorldImagery)\n",
    "    m.add_layer(esri)\n",
    "    \n",
    "    image = ImageOverlay(url=png_name,\n",
    "                         bounds=((latitude[0],longitude[0]),(latitude[1], longitude[1])),\n",
    "                         opacity=0.8)\n",
    "    m.add_layer(image)\n",
    "    \n",
    "    dc = DrawControl(circlemarker={'color': 'yellow'},\n",
    "                    polygon={}, polyline={})\n",
    "    m.add_control(dc)\n",
    "    \n",
    "    m.add_control(LayersControl())\n",
    "    \n",
    "    return m, dc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # display lf_trend\n",
    "# m, pos = display_array_and_sample_point(productivity * 1000, 'lin_trend',\n",
    "#                                         [[-20, 'red'], [0, 'grey'], [20, 'green']])\n",
    "# # display mk_trend\n",
    "# m, pos = display_array_and_sample_point(productivity, 'mk_trend',\n",
    "#                                         [[-1, 'red'], [0, 'grey'], [1, 'green']])\n",
    "# display signif\n",
    "m, pos = display_array_and_sample_point(productivity, 'signif',\n",
    "                                        [[-3, '153, 0, 0'], [-2, '255, 26, 26'], [-1, '255, 153, 153'],\n",
    "                                         [0, 'grey'],\n",
    "                                         [1, '173, 235, 173'], [2, '0, 204, 0'], [3, '0, 77, 0']])\n",
    "m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "location = pos.last_draw['geometry']['coordinates']\n",
    "\n",
    "# Get the location of nearest pixel center and corresponding values\n",
    "pix = productivity.sel(latitude = location[1], longitude = location[0], method = \"nearest\")\n",
    "pix_vals = \"kendall90: {:>3} | lin_trend: {:.4f}\\nkendall95: {:>3} | mk_trend: {}\\nkendall99: {:>3} | signif: {}\" \\\n",
    "      .format(kendall90, pix.lin_trend.values, kendall95, pix.mk_trend.values, kendall99, pix.signif.values)\n",
    "\n",
    "# Get resolutions\n",
    "resx = (NDVI.longitude.values.max() - NDVI.longitude.values.min()) / NDVI.dims['longitude']\n",
    "resy = (NDVI.latitude.values.max() - NDVI.latitude.values.min()) / NDVI.dims['latitude']\n",
    "\n",
    "display_map(latitude = (float(pix.latitude) - (resy /2), float(pix.latitude) + (resy /2)),\n",
    "            longitude = (float(pix.longitude) - (resx /2), float(pix.longitude) + (resx /2)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ndvi_mean_pix = NDVI['mean'].sel(latitude = location[1], longitude = location[0], method = \"nearest\")\n",
    "ndvi_qual_pix = NDVI['qual'].sel(latitude = location[1], longitude = location[0], method = \"nearest\") / 100\n",
    "ndvi_count_pix = NDVI['count'].sel(latitude = location[1], longitude = location[0], method = \"nearest\")\n",
    "intercept_pix = intercept.sel(latitude = location[1], longitude = location[0], method = \"nearest\")\n",
    "\n",
    "# Compute trend line\n",
    "x = ndvi_mean_pix['time'].values\n",
    "y = pix.lin_trend.values * x + intercept_pix.values\n",
    "\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(x, ndvi_mean_pix, label = 'mean NDVI', color = 'green', linewidth = 4)\n",
    "ax1.plot(x, y, \"r--\", label = 'mean NDVI trend', color = 'green' )\n",
    "ax1.plot(x, ndvi_qual_pix, label = 'data percentage', color = 'orange')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.text(0.05, 0.05, pix_vals,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='bottom',\n",
    "        transform = ax1.transAxes,\n",
    "        fontweight='bold',\n",
    "        bbox={'facecolor':'grey', 'alpha':0.5, 'pad':5})\n",
    "ax1.set_ylabel('NDVI/percentage')\n",
    "ax1.xaxis.grid()\n",
    "ax1.yaxis.grid()\n",
    "plt.ylim(0,1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, ndvi_count_pix, label = 'data count', color = 'red')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Count')\n",
    "plt.ylim(0,max(ndvi_count_pix) + 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
